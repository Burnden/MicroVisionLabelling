{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [32, 64, 128]\n",
    "ratios = [0.5, 1, 2]\n",
    "image_shape = [1024, 1024,3]\n",
    "BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "shape = np.array([[int(math.ceil(image_shape[0] / stride)),int(math.ceil(image_shape[1] / stride))] for stride in BACKBONE_STRIDES])\n",
    "feature_stride = [4, 8, 16, 32, 64]\n",
    "anchor_stride = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,\n",
    "                             anchor_stride):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "\n",
    "    Returns:\n",
    "    anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, y2, x2)]\n",
    "    anchors = []\n",
    "    for i in range(len(scales)):\n",
    "        anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride))\n",
    "    return np.concatenate(anchors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    # Enumerate heights and widths from scales and ratios\n",
    "    heights = scales / np.sqrt(ratios)\n",
    "    widths = scales * np.sqrt(ratios)\n",
    "\n",
    "    # Enumerate shifts in feature space\n",
    "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "\n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "\n",
    "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
    "    box_centers = np.stack(\n",
    "        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
    "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
    "\n",
    "    # Convert to corner coordinates (y1, x1, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Base configuration class. For custom configurations, create a\n",
    "    sub-class that inherits from this one and override properties\n",
    "    that need to be changed.\n",
    "    \"\"\"\n",
    "    # Name the configurations. For example, 'COCO', 'Experiment 3', ...etc.\n",
    "    # Useful if your code needs to do things differently depending on which\n",
    "    # experiment is running.\n",
    "    NAME = None  # Override in sub-classes\n",
    "\n",
    "    # NUMBER OF GPUs to use. For CPU training, use 1\n",
    "    GPU_COUNT = 1\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 1000\n",
    "\n",
    "    # Number of validation steps to run at the end of every training epoch.\n",
    "    # A bigger number improves accuracy of validation stats, but slows\n",
    "    # down the training.\n",
    "    VALIDATION_STEPS = 50\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # The strides of each layer of the FPN Pyramid. These values\n",
    "    # are based on a Resnet101 backbone.\n",
    "    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1  # Override in sub-classes\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
    "\n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "    # Anchor stride\n",
    "    # If 1 then anchors are created for each cell in the backbone feature map.\n",
    "    # If 2, then anchors are created for every other cell, and so on.\n",
    "    RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 256\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 2000\n",
    "    POST_NMS_ROIS_INFERENCE = 1000\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and inferencing\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 800\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    # Minimum scaling ratio. Checked after MIN_IMAGE_DIM and can force further\n",
    "    # up scaling. For example, if set to 2 then images are scaled up to double\n",
    "    # the width and height, or more, even if MIN_IMAGE_DIM doesn't require it.\n",
    "    # Howver, in 'square' mode, it can be overruled by IMAGE_MAX_DIM.\n",
    "    IMAGE_MIN_SCALE = 0\n",
    "\n",
    "    # Image mean (Gray)\n",
    "    MEAN_PIXEL = 128\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Percent of positive ROIs used to train classifier/mask heads\n",
    "    ROI_POSITIVE_RATIO = 0.33\n",
    "\n",
    "    # Pooled ROIs\n",
    "    POOL_SIZE = 7\n",
    "    MASK_POOL_SIZE = 14\n",
    "\n",
    "    # Shape of output mask\n",
    "    # To change this you also need to change the neural network mask branch\n",
    "    MASK_SHAPE = [28, 28]\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 100\n",
    "\n",
    "    # Bounding box refinement standard deviation for RPN and final detections.\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3\n",
    "\n",
    "    # Learning rate and momentum\n",
    "    # The Mask RCNN paper uses lr=0.02, but on TensorFlow it causes\n",
    "    # weights to explode. Likely due to differences in optimzer\n",
    "    # implementation.\n",
    "    LEARNING_RATE = 0.001\n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "\n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.0001\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.,\n",
    "        \"rpn_bbox_loss\": 1.,\n",
    "        \"mrcnn_class_loss\": 1.,\n",
    "        \"mrcnn_bbox_loss\": 1.,\n",
    "        \"mrcnn_mask_loss\": 1.\n",
    "    }\n",
    "\n",
    "    # Use RPN ROIs or externally generated ROIs for training\n",
    "    # Keep this True for most situations. Set to False if you want to train\n",
    "    # the head branches on ROI generated by code rather than the ROIs from\n",
    "    # the RPN. For example, to debug the classifier head without having to\n",
    "    # train the RPN.\n",
    "    USE_RPN_ROIS = True\n",
    "\n",
    "    # Train or freeze batch normalization layers\n",
    "    #     None: Train BN layers. This is the normal mode\n",
    "    #     False: Freeze BN layers. Good when using a small batch size\n",
    "    #     True: (don't use). Set layer in training mode even when inferencing\n",
    "    TRAIN_BN = False  # Defaulting to False since batch size is often small\n",
    "\n",
    "    # Gradient norm clipping\n",
    "    GRADIENT_CLIP_NORM = 5.0\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Set values of computed attributes.\"\"\"\n",
    "        # Effective batch size\n",
    "        self.BATCH_SIZE = self.IMAGES_PER_GPU * self.GPU_COUNT\n",
    "\n",
    "        # Input image size\n",
    "        if self.IMAGE_RESIZE_MODE == \"crop\":\n",
    "            self.IMAGE_SHAPE = np.array([self.IMAGE_MIN_DIM, self.IMAGE_MIN_DIM, 1]) ##### editted\n",
    "        else:\n",
    "            self.IMAGE_SHAPE = np.array([self.IMAGE_MAX_DIM, self.IMAGE_MAX_DIM, 1]) ##### editted\n",
    "\n",
    "        # Image meta data length\n",
    "        # See compose_image_meta() for details\n",
    "        self.IMAGE_META_SIZE = 1 + 3 + 3 + 4 + 1 + self.NUM_CLASSES\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backbone_shapes(config, image_shape):\n",
    "    \"\"\"Computes the width and height of each stage of the backbone network.\n",
    "\n",
    "    Returns:\n",
    "        [N, (height, width)]. Where N is the number of stages\n",
    "    \"\"\"\n",
    "    # Currently supports ResNet only\n",
    "    assert config.BACKBONE in [\"resnet50\", \"resnet101\"]\n",
    "    return np.array(\n",
    "        [[int(math.ceil(image_shape[0] / stride)),\n",
    "            int(math.ceil(image_shape[1] / stride))]\n",
    "            for stride in config.BACKBONE_STRIDES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)\n",
    "anchors = generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                         config.RPN_ANCHOR_RATIOS,\n",
    "                                         backbone_shapes,\n",
    "                                         config.BACKBONE_STRIDES,\n",
    "                                         config.RPN_ANCHOR_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261888, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
